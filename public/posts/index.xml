<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Suyog</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Suyog</description>
    <generator>Hugo -- 0.125.7</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Apr 2025 21:06:20 +0545</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Understanding Transformer Architecture from First Principles: A Detailed Exploration</title>
      <link>http://localhost:1313/posts/transformer/</link>
      <pubDate>Mon, 28 Apr 2025 21:06:20 +0545</pubDate>
      <guid>http://localhost:1313/posts/transformer/</guid>
      <description>Introduction Transformer Architecture lies at the heart of modern AI models. Nearly all the state-of-the-art large language models(LLMs) like ChatGPT, LLaMa and Gemini ,all of them are built upon the transformer architecture.The transformer architecture was introduced in the paper Attention is all you need [^1]. Although it is used everywhere these days,it was first introduced for the purpose of language translation but then it was quickly generalized to other task as well.</description>
    </item>
    <item>
      <title>ResNets Explained - Solving Deep Network Degradation with Residual Learning</title>
      <link>http://localhost:1313/posts/resnets/</link>
      <pubDate>Mon, 03 Feb 2025 21:13:03 +0545</pubDate>
      <guid>http://localhost:1313/posts/resnets/</guid>
      <description>Introduction Deep convolutional neural networks have been around for a while and have completely revolutionized how we tackle image recognition task in computer vision. When AlexNet came out in 2012 it revolutionized how we use CNN&amp;rsquo;s as it was the first time we saw an architecture with consecutive Convolutional Layers with significant improvement in training speed and performance. This was achieved by leveraging deeper architecture and GPU accelaration.This 8 layer deep CNN was one of first to have this kind of performance for a largescale image classification.</description>
    </item>
  </channel>
</rss>
