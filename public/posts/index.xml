<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Suyog</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Suyog</description>
    <generator>Hugo -- 0.137.1</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Jan 2025 19:41:43 +0545</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Understanding ResNets: A Comprehensive Guide to Residual Networks.</title>
      <link>http://localhost:1313/posts/resnets/</link>
      <pubDate>Thu, 30 Jan 2025 19:41:43 +0545</pubDate>
      <guid>http://localhost:1313/posts/resnets/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Deep learning has revolutionized fields like image recognition, natural language processing, and speech recognition. While Convolutional Neural Networks (CNNs) have been the go-to architecture for image classification tasks, they come with their own challenges as the network depth increases. One of the biggest challenges faced by deep neural networks is the issue of vanishing or exploding gradients during backpropagation. To address this problem, a breakthrough architecture called &lt;strong&gt;Residual Networks (ResNets)&lt;/strong&gt; was introduced.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
