<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Understanding Transformer Architecture from First Principles: A Detailed Exploration | Suyog</title>
<meta name="keywords" content="">
<meta name="description" content="Introduction Transformer Architecture lies at the heart of modern AI models. Nearly all the state-of-the-art large language models(LLMs) like ChatGPT, LLaMa and Gemini ,all of them are built upon the transformer architecture.The transformer architecture was introduced in the paper Attention is all you need [^1]. Although it is used everywhere these days,it was first introduced for the purpose of language translation but then it was quickly generalized to other task as well.">
<meta name="author" content="Suyog Ghimire">
<link rel="canonical" href="http://localhost:1313/posts/transformer/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fdb895230aa694f0ea1ac2c71f237858b03c86bfd6209d70d1fed9d97c815a4.css" integrity="sha256-j9uJUjCqaU8OoawscfI3hYsDyGv9YgnXDR/tnZfIFaQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/android-chrome-192x192.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/android-chrome-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/android-chrome-192x192.png">
<link rel="apple-touch-icon" href="http://localhost:1313/android-chrome-512x512.png">
<link rel="mask-icon" href="http://localhost:1313/android-chrome-512x512.png">
<meta name="theme-color" content="#ffffff">
<meta name="msapplication-TileColor" content="#ffffff">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/transformer/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Suyog (Alt + H)">Suyog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/Suyog-16" title="Projects">
                    <span>Projects</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://drive.google.com/file/d/1dSxZsK8xPPDeFpjWan3aMsp-Ak6wfPVS/view" title="Resume">
                    <span>Resume</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Understanding Transformer Architecture from First Principles: A Detailed Exploration
    </h1>
    <div class="post-meta"><span title='2025-04-28 21:06:20 +0545 +0545'>April 28, 2025</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;Suyog Ghimire

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#problems-with-rnns-and-sequence-models" aria-label="Problems with RNN&rsquo;s and Sequence models">Problems with RNN&rsquo;s and Sequence models</a><ul>
                        
                <li>
                    <a href="#1-vanishingexploding-gradient-problem" aria-label="1) Vanishing/Exploding Gradient problem">1) Vanishing/Exploding Gradient problem</a></li>
                <li>
                    <a href="#2-handling-long-term-dependencies" aria-label="2) Handling Long term dependencies">2) Handling Long term dependencies</a></li>
                <li>
                    <a href="#3-sequential-processing" aria-label="3) Sequential Processing">3) Sequential Processing</a></li></ul>
                </li>
                <li>
                    <a href="#architecture-overview" aria-label="Architecture Overview">Architecture Overview</a></li>
                <li>
                    <a href="#positional-encoding" aria-label="Positional Encoding">Positional Encoding</a><ul>
                        
                <li>
                    <a href="#" aria-label="Why does this intensity pattern matters?">Why does this intensity pattern matters?</a></li></ul>
                </li>
                <li>
                    <a href="#self-attention-mechanism" aria-label="Self-Attention Mechanism">Self-Attention Mechanism</a><ul>
                        
                <li>
                    <a href="#visualising-self-attention" aria-label="Visualising self-attention">Visualising self-attention</a></li>
                <li>
                    <a href="#mathematical-implementation-of-single-head-self-attention" aria-label="Mathematical implementation of single head self-attention">Mathematical implementation of single head self-attention</a></li>
                <li>
                    <a href="#problem-with-single-head-self-attention" aria-label="Problem with single-head self-attention">Problem with single-head self-attention</a></li></ul>
                </li>
                <li>
                    <a href="#multi-head-attention" aria-label="Multi-Head Attention">Multi-Head Attention</a><ul>
                        
                <li>
                    <a href="#self-attention-per-head" aria-label="Self-attention per Head">Self-attention per Head</a></li></ul>
                </li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Transformer Architecture lies at the heart of modern AI models. Nearly all the state-of-the-art large language models(LLMs) like ChatGPT, LLaMa and  Gemini ,all of them are built upon the transformer architecture.The transformer architecture was introduced in the paper <strong>Attention is all you need</strong> [^1]. Although it is used everywhere these days,it was first introduced for the purpose of language translation but then it was quickly generalized to other task as well.Since then, due to its scalability and self-attention mechanism it has found itself in not just natural language processing(NLP) but also in computer vision,speech and multimodal AI systems.</p>
<p>In this blog, we will explore the internals of the Transformer Architecture ,with mathematical intuition and understand why it became the backbone of generative AI.</p>
<h2 id="problems-with-rnns-and-sequence-models">Problems with RNN&rsquo;s and Sequence models<a hidden class="anchor" aria-hidden="true" href="#problems-with-rnns-and-sequence-models">#</a></h2>
<p>Before transformers, many natural language processing task were done using sequence models like recurrent neural network(RNN&rsquo;s).RNNs had major flaws due to which it was not good enough for most language-based applications.Lets discuss their flaws and how transformer massively improved upon them</p>
<h3 id="1-vanishingexploding-gradient-problem">1) Vanishing/Exploding Gradient problem<a hidden class="anchor" aria-hidden="true" href="#1-vanishingexploding-gradient-problem">#</a></h3>
<p>One of the major problems with RNNs were the vanishing/exploding gradients.
<img src="/images/rnn.webp" alt="Description" 
style="width: 100%; height: 300px; object-fit: cover; border-radius: 8px;"></p>
<center><i>Unfolding of Recurrent Neural Network</i></center>
<p><em>Credit: <a href="https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-1/">dennybritz</a>.</em></p>
<p>The above diagram shows an RNN being unfolded/unrolled across time steps.We essentially have a full network for all the input sequence.For a sequence of 10 words/tokens the network would be unrolled into a 10 layer network one for each time step.This is fine for smaller sequences but for longer input sequences,the network becomes very deep</p>
<p>As a result during, backpropagation through time(BPTT) the gradient at each step would propagate backwards through many steps, this could cause the gradient to be extremely small causing a <strong>Vanishing Gradient Problem</strong> or the gradient could become very large which would cause the weights to be updated by large amount leading to numerical overflow causing a <strong>Exploding Gradient Problem</strong>.</p>
<h3 id="2-handling-long-term-dependencies">2) Handling Long term dependencies<a hidden class="anchor" aria-hidden="true" href="#2-handling-long-term-dependencies">#</a></h3>
<center><b> "Shyam grew up in Nepal where he stayed until he was 20 years old therefore he speaks fluent _____." </b></center>
<br>
<p>The correct completion is &ldquo;Nepali&rdquo; but for the model to correctly predict it. It needs to store the context word &ldquo;Nepal&rdquo; which appears 15-20 tokens before the target, diluted by many intermediate states. without the context of &ldquo;Nepal&rdquo; the model has no clue to guess the target</p>
<h3 id="3-sequential-processing">3) Sequential Processing<a hidden class="anchor" aria-hidden="true" href="#3-sequential-processing">#</a></h3>
<p>RNN&rsquo;s rely on sequential processing, meaning the <b>next output at each step depends on the previous step.</b> Mathematically, for seqeunce of $x_1, x_2, x_3, \ldots, x_n$</p>
<p>$$ h_{t} = f(h_{t-1},x_{t}) $$
where hidden state at time step $t$ depends upon previous step. we cant calculate $h_t$ without calculating $h_{t-1}$ first. Unlike CNNs or Transformers where many computations could occur simultaneously. RNNs cannot make use of parallel computations, they are stuck in a chain-like process.
<br>
<br>
Because of these limitations it was of no use when it came to handling large seqential data, this makes them slow,ineffective and limited. These problems are completely solved in modern architectures like Transformers</p>
<h2 id="architecture-overview">Architecture Overview<a hidden class="anchor" aria-hidden="true" href="#architecture-overview">#</a></h2>
<p>Lets look at the Overall architecture of Transformer model and go into each section and dicuss that in detailed later in the blog</p>
<p><img src="/images/transformers/transformer.png" alt="Description" 
style="width: 100%; height: 800px; object-fit: cover; border-radius: 8px;"></p>
<p>[^1]<center><i>Transformer Architecture</i></center></p>
<p>Transformer architecture consist of two main components: <strong>encoder</strong> and a <strong>decoder</strong>. The encoder takes in an input sequence(tokens) and converts it into numerical vectors that captures its meaning and relationship with each other. The decoder uses these encoded representations along with the tokens it has already generated to provide us with a probabilty distribution of possible next tokens at each time step.</p>
<p>We will get into the details of each and every step later in this blog.</p>
<h2 id="positional-encoding">Positional Encoding<a hidden class="anchor" aria-hidden="true" href="#positional-encoding">#</a></h2>
<p>Unlike CNNs and RNNs, Transformers which uses self attention  is <strong>Permutation Invaraint</strong> meaning a it doesnt care about what order the tokens came in. It treats them as a bag of vector. For example a input sequence &quot; <strong>Ram pushed Hari</strong>&quot; and &ldquo;<strong>Hari pushed Ram</strong>&rdquo; would look the same to the transformer without positional information.</p>
<p>To address this we inject position information to the input sequence embeddings before feeding them to the transformer layers.</p>
<p>We want the model to identify which tokens are nearer and which are futher distant. <strong>Note</strong>: These are not learnable positional encodings rather static</p>
<p>Each input token is first mapped to a vector embedding of dimension 512(in the paper)
<img src="/images/transformers/vector.png" alt="Description" 
style="width: 100%; height: 350px; object-fit: cover; border-radius: 8px;">
<em>Credit: <a href="">Author</a>.</em></p>
<center><i>Input vector encodings</i> </center>
<br>
<p>Above figure shows how each input token is mapped to a vector of a fixed dimension but it lacks positional information.Its solution lies in adding another vector <strong>positional encoding vector</strong> of same dimension(512) to the original vector embedding.</p>
<p>$$ E_{final}(pos,token) = E_{word}(token) + PE(pos)$$</p>
<p>Where:
<br></p>
<ul>
<li>$E_{word}(token)$ is the 512 dimensional token embedding(as shown in the diagram)
<br></li>
<li>$PE(pos)$ is the positonal encoding vector for position $pos$</li>
</ul>
<p>To calculate this we have positonal encoding fourmula as:
$$PE(pos, 2i) = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})$$</p>
<p>$$PE(pos, 2i+1) = cos(\frac {pos}{10000^{\frac{2i}{d_{model}}}})$$</p>
<p>Where:
<br></p>
<ul>
<li>$pos$ = token position(0,1,2&hellip;.)
<br></li>
<li>$i$ = dimension index(0 to 255 for 512-dimension embeddings)
<br></li>
<li>$d_{model}$ = embedding dimesion(512)</li>
</ul>
<p>So odd dimensions(0,2,4,&hellip;) takes the cosine fourmula whereas even dimension(1,3,5,..) takes sine fourmula
<img src="/images/transformers/pos.png" alt="Description" 
style="width: 100%; height: 670px; object-fit: cover; border-radius: 8px;">
<em>Credit: <a href="">Author</a>.</em></p>
<center><i>Positional encoding calculations</i> </center>
<br>
<p>Now the question aries on how does adding the positional encodings generated by these trignometric fourmula help the model gain positional information on input sequence.</p>
<p><img src="/images/transformers/positional_encoding.webp" alt="Description" 
style="width: 100%; height: 450px; object-fit: cover; border-radius: 8px;"></p>
<p><em>Credit: <a href="https://www.scaler.com/topics/nlp/positional-encoding/">sercaler</a>.</em></p>
<center><i>Position vs  embed_dimension heat map </i> </center>
<br>
<p>In the above chart were X-axis represents encoding dimensions and Y-axis represents different position in sequence,a particular cell&rsquo;s intensity is the sine/cosine value.
we can clearly see a visible pattern in this chart the left side <strong>High intensity changes</strong> creates a checkerboard pattern and on the right side <strong>Smooth intensity changes</strong>creates a smooth gradients.</p>
<center><h3>Why does this intensity pattern matters?</h3></center>
<p><strong>Rapid color changes</strong> means that model can distinguish between nearby positions and <strong>smooth color changes</strong> means model understands broad position relationships, with this each position gets a unique fingerprint.</p>
<p>Different frequencies of sine/cosine functions create the intensity variations you see, giving each position a unqiue pattenr across all dimensions</p>
<h2 id="self-attention-mechanism">Self-Attention Mechanism<a hidden class="anchor" aria-hidden="true" href="#self-attention-mechanism">#</a></h2>
<p>Before going deep towards the mathematics of self-attention lets first look self- attention from a higher level.Self attention lets each token look at every other token in the seqeuence and determine how much each if them matters when building new representations.The &ldquo;<strong>self</strong>&rdquo; in self-attention simply refers to the the fact that it uses the surrounding words within the sequence to provide context.This can all be done in parallel which can leverage parallel processing for faster computations.</p>
<h3 id="visualising-self-attention">Visualising self-attention<a hidden class="anchor" aria-hidden="true" href="#visualising-self-attention">#</a></h3>
<p>Simply speaking the goal of self attention is to move/change the vector embedding for each token to a embedding vector spcae that better represents the context. For example: we have a word/token &ldquo;<strong>Apple</strong>&rdquo;, now apple has mutilple meanings,it could be the fruit apple or the company apple that makes iphones. Now if we visualize this in a <strong>2-Dimensional vector space</strong> it would probably lean more towards the fruit clusters. Now if theres a context say u ask</p>
<center><b>"Explain me about the new apple devices."</b></center>
<p>Now since apple has multiple meanings , since we are talking about the <strong>tech company</strong> &ldquo;Apple&rdquo;.Now the word/token apple must be shifted towards the technological side from the fruits/juices side in the embedding vector space.Below is a 2-Dimensional visualization for this.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create word embeddings</span>
</span></span><span style="display:flex;"><span>xs <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">1.5</span>, <span style="color:#ae81ff">2.5</span>, <span style="color:#ae81ff">6.0</span>, <span style="color:#ae81ff">7.5</span>, <span style="color:#ae81ff">8.0</span>]
</span></span><span style="display:flex;"><span>ys <span style="color:#f92672">=</span> [<span style="color:#ae81ff">3.0</span>, <span style="color:#ae81ff">1.2</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">8.0</span>, <span style="color:#ae81ff">7.5</span>, <span style="color:#ae81ff">5.5</span>]
</span></span><span style="display:flex;"><span>words <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;fruit&#39;</span>, <span style="color:#e6db74">&#39;tree&#39;</span>, <span style="color:#e6db74">&#39;juice&#39;</span>, <span style="color:#e6db74">&#39;mac&#39;</span>, <span style="color:#e6db74">&#39;os&#39;</span>, <span style="color:#e6db74">&#39;app&#39;</span>]
</span></span><span style="display:flex;"><span>apple <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">4.5</span>, <span style="color:#ae81ff">4.5</span>], [<span style="color:#ae81ff">6.7</span>, <span style="color:#ae81ff">6.5</span>]]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create figure</span>
</span></span><span style="display:flex;"><span>fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(ncols<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add titles</span>
</span></span><span style="display:flex;"><span>ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Learned Embedding for &#34;apple&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">without context&#39;</span>)
</span></span><span style="display:flex;"><span>ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Contextual Embedding for</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;apple&#34; after self-attention&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add trace on plot 2 to show the movement of &#34;apple&#34;</span>
</span></span><span style="display:flex;"><span>ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>scatter(apple[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>], apple[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span>, s<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>)
</span></span><span style="display:flex;"><span>ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>plot([apple[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]<span style="color:#f92672">+</span><span style="color:#ae81ff">0.1</span>, apple[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>]],
</span></span><span style="display:flex;"><span>           [apple[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>]<span style="color:#f92672">+</span><span style="color:#ae81ff">0.1</span>, apple[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">1</span>]],
</span></span><span style="display:flex;"><span>           linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;dashed&#39;</span>,
</span></span><span style="display:flex;"><span>           zorder<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">2</span>):
</span></span><span style="display:flex;"><span>    ax[i]<span style="color:#f92672">.</span>set_xlim(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>    ax[i]<span style="color:#f92672">.</span>set_ylim(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Plot word embeddings</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (x, y, word) <span style="color:#f92672">in</span> list(zip(xs, ys, words)):
</span></span><span style="display:flex;"><span>        ax[i]<span style="color:#f92672">.</span>scatter(x, y, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>, s<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>        ax[i]<span style="color:#f92672">.</span>text(x<span style="color:#f92672">+</span><span style="color:#ae81ff">0.5</span>, y, word)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Plot &#34;apple&#34; vector</span>
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> apple[i][<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> apple[i][<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;blue&#39;</span> <span style="color:#66d9ef">if</span> i <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#39;purple&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    ax[i]<span style="color:#f92672">.</span>text(x<span style="color:#f92672">+</span><span style="color:#ae81ff">0.5</span>, y, <span style="color:#e6db74">&#39;apple&#39;</span>)
</span></span><span style="display:flex;"><span>    ax[i]<span style="color:#f92672">.</span>scatter(x, y, c<span style="color:#f92672">=</span>color, s<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img src="/images/transformers/self.png" alt="Description" 
style="width: 100%; height: 400px; object-fit: cover; border-radius: 8px;">
<em>Credit: <a href="">Author</a>.</em></p>
<center><i>Contextual embedding before /after</i> </center>
<br>
We can clearly see the token "apple" shift itself according to the context in the vector embedding space due to self-attention.
<h3 id="mathematical-implementation-of-single-head-self-attention">Mathematical implementation of single head self-attention<a hidden class="anchor" aria-hidden="true" href="#mathematical-implementation-of-single-head-self-attention">#</a></h3>
<p>Now, we have seen what self-attention is and how it works at a higher level, we will dive deep into how it is implemented mathematically.In this simple case lets consider the sequence we used earlier</p>
<center><b> "I" "love" "to" "eat" "pizza" </b></center>
<br>
then
sequence length $seq$ = 5 and $d_{model}$= 512(from vanilla transformer)
<p>we obtain a input embedding matrix after positional encoding step as such:</p>
<p><img src="/images/transformers/input_embed.png" alt="Description" 
style="width: 100%; height: 550px; object-fit: cover; border-radius: 8px;">
<em>Credit: <a href="">Author</a>.</em></p>
<center><i>Input embedding matrix</i> </center>
<br>
here, $$ X\in \mathbb{R}^{5\times512}$$
<p>for a single head self attention we have to first project the matrix $X$ into three different matrices,<strong>key, query and value</strong>. these are linear projections of input embeddings obtain by mutlipy with <strong>three different weight matrices</strong>
$$
W_Q \in \mathbb{R}^{512 \times d_k}$$
$$W_K \in \mathbb{R}^{512 \times d_k}$$
$$W_V \in \mathbb{R}^{512 \times d_v}
$$</p>
<p>then finally u compute matrices <strong>Q,K,V</strong> by multiply these learnable weight matrices to the input embedding matrix:</p>
<p>$$ Q = X . W_Q   $$
$$ K = X . W_K  $$
$$ V = X . W_V  $$
then for this case as $X$ is $(5\times512$) we choose
$$ d_k = d_v = 512$$</p>
<img src="/images/transformers/projection.png" alt="projection" style="width: 100%; height: auto;"> 
<p><em>Credit: <a href="">Author</a>.</em></p>
<center><i>Creation of query,key and Value matrices through linear projection.</i> </center>
<br>
<p>The obtained <strong>Query,Key and Value</strong> matrices are now used to calculate the attention score through self-attention fourmula
$$Attention(Q,K,V) = softmax\Big(\frac{QK^T}{\sqrt{d_k}}\Big) V$$
From this fourmula,before we get attention scores we first calculate similarity scores between tokens using query and key vectors:
$$ Similarity = QK^T$$
Raw similarity scores can be +ve,-ve , large or small, higher similarity means words.token are more related . These similarity scores are scaled then passed through softmax to normalize all scores to sum to <strong>1.0</strong>. It essentially converts raw similarity to probability like weights.</p>
<p>The output after applying this fourmula would be a attention score matrix of size $5\times 5$. where the attention score determine how much each word/token <strong>pays attention</strong> to every other word including itself.The matrix will look something like this:</p>
<img src="/images/transformers/attention_output.png" alt="projection" style="width: 100%; height: auto;">
<p><em>Credit: <a href="">Author</a>.</em></p>
<center><i>Attention score matrix(approximation only)</i> </center>
<br>
<p>Each row become as probability distribution whose sum is <strong>1.0</strong>.Each cell shows attention score between two words</p>
<p><strong>What we can observe</strong> is that
every token pays most attention to itself, as seen in the diagonal. we can also see <strong>&lsquo;pizza&rsquo;</strong> has the strongest self-attention and connects strongly to <strong>&rsquo;eat&rsquo;</strong> which shows how model has understood relationships between words, subjects connect to verbs, verbs connect to objects.</p>
<h3 id="problem-with-single-head-self-attention">Problem with single-head self-attention<a hidden class="anchor" aria-hidden="true" href="#problem-with-single-head-self-attention">#</a></h3>
<p>Single head works but plateaus in performance as a single head self-attention only limits us to a single view of similarity. Natural language is <strong>rich and ambigiuos</strong> , a single sentence carries multiple layers of information simultaneously such as</p>
<ul>
<li>Syntatic strcuture- which words are subject,verbs,objects,etc</li>
<li>Semantic relationship - which words relate in meaning(&ldquo;dog&rdquo; &ldquo;barks&rdquo;)</li>
<li>Contextual meanings - sentiment(&ldquo;happy&rdquo; might mean smile)</li>
</ul>
<p>A single head cannot capture all these aspect limiting us, so to tackle this problem multi-head attention is used</p>
<h2 id="multi-head-attention">Multi-Head Attention<a hidden class="anchor" aria-hidden="true" href="#multi-head-attention">#</a></h2>
<p>Since, we already discuss the problem with single head self-attention lets discuss how multi-head attention mechanism works.</p>
<p><strong>Multi-Head Attention</strong> splits the attention mechanims into $H$(H=8 in paper) independent heads, each learning its own smaller set of learnable parameters in parallel and concating them at last.Each head works in a smaller subspace
$$ d_k = d_v = \frac{d_{model}}{h} = 512/8 = 64$$
dimension of each head becomes 64.The spliting happens in the following fashion</p>
<img src="/images/transformers/multihead.png" alt="projection" style="width: 100%; height: auto;">
<p><em>Credit: <a href="">Author</a>.</em></p>
<center><i>Multi-head spliting</i> </center>
<br>
For multi-head attention, we divide each of Q,K,and V into h = 8 heads. Each head operated on a smaller dimension so,
<p>$$ Q \to [Q^{(1)},Q^{(2)},&hellip;.,Q^{(8)}]  , Q^{(i)}\in \mathbb{R}^{5\times 64}$$
Similarly for K,V</p>
<h3 id="self-attention-per-head">Self-attention per Head<a hidden class="anchor" aria-hidden="true" href="#self-attention-per-head">#</a></h3>
<p>$$  Attention(Q^{(i)},K^{(i)},V^{(i)})= softmax\Big(\frac{Q^{(i)}{K^{(i)}}^T}{\sqrt{64}}\Big)V^{(i)} $$</p>
<p>Output of each head:
$$ H^{(i)} \in \mathbb{R}^{5\times 64} $$</p>
<img src="/images/transformers/single_head.png" alt="projection" style="width: 100%; height: auto;">
<p><em>Credit: <a href="">Author</a>.</em></p>
<center><i>Calculation of attention score for single head</i> </center>
<br>
<p>At last all output heads are concatenated:
$$ H = [H^{(1)} | H^{(2)}| &hellip;..|H^{(8)}]$$
where $ H \in \mathbb{R}^{5\times 512}$</p>
<img src="/images/transformers/concat.png" alt="projection" style="width: 100%; height: auto;">
<p><em>Credit: <a href="">Author</a>.</em></p>
<center><i>Concatenation of all heads</i> </center>
<br>
<p>Finally a linear projection is applied to bring to the original model dimension:
$$ Multihead(X) = H W_o$$</p>
<p>where:
$ W_o \in \mathbb{R}^{512\times512} $</p>
<img src="/images/transformers/final_projection.png" alt="projection" style="width: 100%; height: auto;">
<p><em>Credit: <a href="">Author</a>.</em></p>
<center><i>Final projection</i> </center>
<br>
<p>Thus, the final output of multi-head attention is obtained. The output dimensions might look the same as single-head self attention but it now carries a deeper more rich representation of input which helps to improve its performance even further.But this isnt the full picture, we still need to look at feed-forward layers,resisdual connection ,layer normalization.</p>
<p>To be continued&hellip;.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &amp; Polosukhin, I. (2017). <em>Attention is all you need</em>. In Advances in Neural Information Processing Systems (NeurIPS).</p>
</li>
<li>
<p>Smith, B. (2024, February 9). <em>Contextual transformer embeddings using self-attention explained with diagrams and Python code</em>. Towards Data Science. <a href="https://towardsdatascience.com/contextual-transformer-embeddings-using-self-attention-explained-with-diagrams-and-python-code-d7a9f0f4d94e/">https://towardsdatascience.com/contextual-transformer-embeddings-using-self-attention-explained-with-diagrams-and-python-code-d7a9f0f4d94e/</a></p>
</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="http://localhost:1313/posts/resnets/">
    <span class="title"> »</span>
    <br>
    <span>ResNets Explained - Solving Deep Network Degradation with Residual Learning</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Understanding Transformer Architecture from First Principles: A Detailed Exploration on x"
            href="https://x.com/intent/tweet/?text=Understanding%20Transformer%20Architecture%20from%20First%20Principles%3a%20A%20Detailed%20Exploration&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Understanding Transformer Architecture from First Principles: A Detailed Exploration on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer%2f&amp;title=Understanding%20Transformer%20Architecture%20from%20First%20Principles%3a%20A%20Detailed%20Exploration&amp;summary=Understanding%20Transformer%20Architecture%20from%20First%20Principles%3a%20A%20Detailed%20Exploration&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Understanding Transformer Architecture from First Principles: A Detailed Exploration on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer%2f&title=Understanding%20Transformer%20Architecture%20from%20First%20Principles%3a%20A%20Detailed%20Exploration">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Understanding Transformer Architecture from First Principles: A Detailed Exploration on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Understanding Transformer Architecture from First Principles: A Detailed Exploration on whatsapp"
            href="https://api.whatsapp.com/send?text=Understanding%20Transformer%20Architecture%20from%20First%20Principles%3a%20A%20Detailed%20Exploration%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Understanding Transformer Architecture from First Principles: A Detailed Exploration on telegram"
            href="https://telegram.me/share/url?text=Understanding%20Transformer%20Architecture%20from%20First%20Principles%3a%20A%20Detailed%20Exploration&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Understanding Transformer Architecture from First Principles: A Detailed Exploration on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Understanding%20Transformer%20Architecture%20from%20First%20Principles%3a%20A%20Detailed%20Exploration&u=http%3a%2f%2flocalhost%3a1313%2fposts%2ftransformer%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Suyog</a></span> · 

    <span>
        
        
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
